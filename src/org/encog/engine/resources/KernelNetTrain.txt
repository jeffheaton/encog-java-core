#define POSITIVE_ETA 1.2f
#define NEGATIVE_ETA 0.5f
#define DELTA_MIN 0.00001f
#define MAX_STEP 50.0f	

float activationTANH(float x, float slope)
{
	float z = exp(-slope * x);
	return (1.0f - z) / (1.0f + z);
}

float derivativeTANH(float x, float slope)
{
	return (slope * (1.0f - x * x));
}

float activationLinear(float x, float slope)
{
	return x*slope;
}

float derivativeLinear(float x, float slope)
{
	return 1;
}

float activationSigmoid(float x, float slope)
{
	return 1.0f / (1.0f + exp(-slope * x));
}

float derivativeSigmoid(float x, float slope)
{
	return slope * x * (1.0f - x);
}

float activation(int type, float x, float slope)
{
	switch(type)
	{
		case 0:
			return activationLinear(x,slope);
		case 1:
			return activationTANH(x,slope);
		case 2:
			return activationSigmoid(x,slope);
		default:
			return 0;
	}
}

float derivative(int type, float x, float slope)
{
	switch(type)
	{
		case 0:
			return derivativeLinear(x,slope);
		case 1:
			return derivativeTANH(x,slope);
		case 2:
			return derivativeSigmoid(x,slope);
		default:
			return 0;
	}
}

int testSign(float v)
{
	//return sign(v);
	
	if( fabs(v)<0.00001f )
	{
		return 0;
	}
	else if( v>0 )
	{
		return 1;
	}
	else 
	{
		return -1;
	}
}

kernel void NetworkTrain(
    global read_only int *params,
    global write_only float *errors,
    global read_only int *layerIndex,
    global read_only int *layerCounts,
    global read_only int *layerFeedCounts,
    global read_only int *weightIndex,
    global read_only float* input,
    global read_only float* ideal,
    global read_only float* weightsIn,
    global write_only float* weightsOut,
    global write_only float *gradients,
    global read_only int *activationType,
    global read_only float *slope,
    global read_only float *tempDataIn,
    global read_only float *tempDataOut
    )
{
	private float layerOutput[NEURON_COUNT];
	private float layerDelta[NEURON_COUNT];
			
	int taskIndex = get_global_id(0);
	int inputSize = params[0];
    int outputSize = params[1];
    int layerCount = params[2];
    int indexLastElement = params[6];
    int sizeElement = params[7];
    int sizeLastElement = params[8];

	int gradientOffset = (taskIndex*WEIGHT_COUNT);

	int size;
	if( taskIndex==indexLastElement )
		size = sizeLastElement;
	else
		size = sizeElement;
		
	// clear out the gradients and errors
	errors[taskIndex] = 0;
	
	for(int trainIndex=0;trainIndex<size;trainIndex++)
	{		 
		int subtaskIndex = (taskIndex*sizeElement)+trainIndex;
		
		// part 1: forward pass
		int taskInputIndex = subtaskIndex * inputSize;
		int taskIdealIndex = subtaskIndex * outputSize;
	
		int sourceIndex = NEURON_COUNT - layerCounts[layerCount-1];
		
		for(int i=0;i<NEURON_COUNT;i++)
			layerOutput[i] = 1;
		
		// load the input into the layer output array, this feeds the first layer.
		for(int i=0;i<inputSize;i++)
			layerOutput[sourceIndex+i] = input[taskInputIndex+i];
				
		for (int currentLayer = layerCount - 1; currentLayer > 0; currentLayer--)
		{
			int inputIndex = layerIndex[currentLayer];
			int outputIndex = layerIndex[currentLayer - 1];
			int inputSize = layerCounts[currentLayer];
			int outputSize = layerFeedCounts[currentLayer - 1];
			int index = weightIndex[currentLayer - 1];

			for (int x = 0; x < outputSize; x++)
			{
				float sum = 0;
				for (int y = 0; y < inputSize; y++)
				{
					sum += weightsIn[index++] * layerOutput[inputIndex + y];
				}
       
				layerOutput[outputIndex + x] = activation(activationType[0], sum, slope[0]);
			}
		}
		
		// part 2: backward pass

		// process the output layer first
	
		float e = 0;
   
		for(int i=0;i<outputSize;i++)
		{
			float diff = ideal[taskIdealIndex+i] - layerOutput[i];
			e+=diff*diff;
			layerDelta[i] = diff * derivative(activationType[0],layerOutput[i], slope[0]);
		}

		errors[taskIndex] += e;

		// process hidden layers
		
		for(int currentLevel = 0; (currentLevel<layerCount-1); currentLevel++)
		{
            int fromLayerIndex = layerIndex[currentLevel + 1];
            int toLayerIndex = layerIndex[currentLevel];
            int fromLayerSize = layerCounts[currentLevel + 1];
            int toLayerSize = layerFeedCounts[currentLevel];

			int index = weightIndex[currentLevel];

			// handle weights
			int yi = fromLayerIndex;
			for (int y = 0; y < fromLayerSize; y++) 
			{
				float output = layerOutput[yi];
				float sum = 0;
				int xi = toLayerIndex;
				int wi = index+y;
				for (int x = 0; x < toLayerSize; x++) 
				{
					gradients[wi+gradientOffset] = output * layerDelta[xi];
					sum += weightsIn[wi] * layerDelta[xi];
					wi+=fromLayerSize;
					xi++;
				}
			
				layerDelta[yi] = sum * derivative(
                  activationType[currentLevel+1],
                  layerOutput[yi],
                  slope[currentLevel+1]);

				yi++;
			}
		}
	}	
	
	// now that the gradients have been calculated, update the network
	barrier(CLK_GLOBAL_MEM_FENCE);
	
	if( taskIndex==0 )
	{
		// loop over all gradients and sum them into the first subtask
		for(int i=0;i<WEIGHT_COUNT;i++) 
		{
			for(int j=1;j<=indexLastElement;j++)
			{		 
				gradients[i] += gradients[(j*WEIGHT_COUNT)+i];
			}
		}

		// teach the weights
#ifdef LEARN_RPROP		
		for(int i=0;i<WEIGHT_COUNT;i++)
		{
			int change = testSign(gradients[i] * tempDataIn[i]);
			float weightChange = 0;

			// if the gradient has retained its sign, then we increase the
			// delta so that it will converge faster
			if (change > 0) 
			{
				float delta = tempDataIn[i+WEIGHT_COUNT]
					* POSITIVE_ETA;
				delta = min(delta, MAX_STEP);
				weightChange = testSign(gradients[i]) * delta;
				tempDataIn[i+WEIGHT_COUNT] = delta;
				tempDataIn[i] = gradients[i];
			}
			else if (change < 0) 
			{
				// if change<0, then the sign has changed, and the last
				// delta was too big
				float delta = tempDataIn[i+WEIGHT_COUNT]
					* NEGATIVE_ETA;
				delta = max(delta, DELTA_MIN);
				tempDataIn[i+WEIGHT_COUNT] = delta;
				// set the previous gradient to zero so that there will be no
				// adjustment the next iteration
				tempDataIn[i] = 0;
			} 
			else if (change == 0) 
			{
				// if change==0 then there is no change to the delta
				float delta = tempDataIn[i+WEIGHT_COUNT];
				weightChange = testSign(gradients[i]) * delta;
				tempDataIn[i] = gradients[i];
			}
			
			weightsIn[i]+=weightChange;
		}	
#endif
#ifdef LEARN_BPROP		
		for(int i=0;i<WEIGHT_COUNT;i++)
		{
			float delta = (gradients[i]*tempDataIn[0]);
			weightsIn[i]+=delta+(tempDataIn[i+2]*tempDataIn[1]);
			tempDataIn[i+2] = delta;
		}
#endif
#ifdef LEARN_MANHATTAN
		for(int i=0;i<WEIGHT_COUNT;i++)
		{
			int direction = testSign(gradients[i]);
			weightsIn[i]+=tempDataIn[0]*direction;
		}
#endif
	}
	
	barrier(CLK_GLOBAL_MEM_FENCE);
	if( taskIndex==0 )
	{
		// finally, after all is done, return the weights to the CPU
		for(int i=0;i<WEIGHT_COUNT;i++)
		{
			weightsOut[i] = weightsIn[i];
		}
		
		for(int i=0;i<(WEIGHT_COUNT*2);i++)
		{
			tempDataOut[i] = tempDataIn[i];
		}
	}
}