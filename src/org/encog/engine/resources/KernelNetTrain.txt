#ifndef USE_SIGMOID 
#ifndef USE_TANH
float activationTANH(float x, float slope)
{
	float z = exp(-slope * x);
	return (1.0f - z) / (1.0f + z);
}

float derivativeTANH(float x, float slope)
{
	float out = activationTANH(x,slope);
	return (slope * (1.0f - out * out));
}

float activationLinear(float x, float slope)
{
	return x*slope;
}

float derivativeLinear(float x, float slope)
{
	return 1;
}

float activationSigmoid(float x, float slope)
{
	return 1.0f / (1.0f + exp(-slope * x));
}

float derivativeSigmoid(float x, float slope)
{
	float out = activationSigmoid(x,slope);
	return slope * out * (1.0f - out);
}

float activation(int type, float x, float slope)
{
	switch(type)
	{
		case 0:
			return activationLinear(x);
		case 1:
			return activationTANH(x);
		case 2:
			return activationSigmoid(x);
		default:
			return 0;
	}
}

float derivative(int type, float x, float slope)
{
	switch(type)
	{
		case 0:
			return derivativeLinear(x);
		case 1:
			return derivativeTANH(x);
		case 2:
			return derivativeSigmoid(x);
		default:
			return 0;
	}
}
#endif
#endif

#ifdef USE_SIGMOID
float activation(int t, float x, float slope)
{
	return 1.0f / (1.0f + exp(-slope * x));
}

float derivative(int t, float x, float slope)
{
	float out = activation(t,x,slope);
	return slope * out * (1.0f - out);
}
#endif

#ifdef USE_TANH
float activation(int t, float x, float slope)
{
	float z = exp(-slope * x);
	return (1.0f - z) / (1.0f + z);
}

float derivative(int t, float x, float slope)
{
	float out = activation(t,x,slope);
	return (slope * (1.0f - out * out));
}
#endif

kernel void NetworkTrain(
    global read_only int *params,
    global write_only float *errors,
    global read_only int *layerIndex,
    global read_only int *layerCounts,
    global read_only int *layerFeedCounts,
    global read_only int *weightIndex,
    global read_only float* input,
    global read_only float* ideal,
    global read_only float* weightsIn,
    global write_only float *gradients,
    global read_only int *activationType,
    global read_only float *slope
    )
{
	private float layerOutput[NEURON_COUNT];
	private float layerDelta[NEURON_COUNT];
	
	local float weights[WEIGHT_COUNT];
	
	event_t event = async_work_group_copy((__local float*)weights, (const __global float*)weightsIn, (size_t)WEIGHT_COUNT, (event_t)0);
		
	int taskIndex = get_global_id(0);

	int inputSize = params[0];
    int outputSize = params[1];
    int layerCount = params[2];
    int indexLastElement = params[6];
    int sizeElement = params[7];
    int sizeLastElement = params[8];

	int gradientOffset = (taskIndex*WEIGHT_COUNT);

	int size;
	if( taskIndex==indexLastElement )
		size = sizeLastElement;
	else
		size = sizeElement;
		
		// clear out the gradients and errors
	for(int i=0;i<WEIGHT_COUNT;i++)
		gradients[gradientOffset+i]=0;
	errors[taskIndex] = 0;

	wait_group_events(1,&event);
	
	for(int index=0;index<size;index++)
	{		
		int subtaskIndex = (taskIndex*sizeElement)+index;
		
		// part 1: forward pass
		int taskInputIndex = subtaskIndex * inputSize;
		int taskIdealIndex = subtaskIndex * outputSize;
	
		int sourceIndex = NEURON_COUNT - layerCounts[layerCount-1];
		
		for(int i=0;i<NEURON_COUNT;i++)
			layerOutput[i] = 1;
		
		// load the input into the layer output array, this feeds the first layer.
		for(int i=0;i<inputSize;i++)
			layerOutput[sourceIndex+i] = input[taskInputIndex+i];
			
		
		for (int currentLayer = layerCount - 1; currentLayer > 0; currentLayer--)
		{
			int inputIndex = layerIndex[currentLayer];
			int outputIndex = layerIndex[currentLayer - 1];
			int inputSize = layerCounts[currentLayer];
			int outputSize = layerFeedCounts[currentLayer - 1];
			int index = weightIndex[currentLayer - 1];

			// bias values
			for (int i = 0; i < outputSize; i++)
			{
				layerOutput[ i + outputIndex] = 0;
			}

			for (int x = 0; x < outputSize; x++)
			{
				float sum = 0;
				for (int y = 0; y < inputSize; y++)
				{
					sum += weights[index++] * layerOutput[inputIndex + y];
				}
       
				float value = layerOutput[outputIndex + x] + sum;
				value = activation(activationType[0], value, 1);
				layerOutput[outputIndex + x] = value;
			}
		}
		
				// part 2: backward pass

		// process the output layer first
	
		float e = 0;
   
		for(int i=0;i<outputSize;i++)
		{
			float diff = ideal[taskIdealIndex+i] - layerOutput[i];
			e+=diff*diff;
			layerDelta[i] = diff * derivative(activationType[0],layerOutput[i], 1);
		}

		errors[taskIndex] += e;

		// process hidden layers
		
		for(int currentLevel = 0; (currentLevel<layerCount-1); currentLevel++)
		{
            int fromLayerIndex = layerIndex[currentLevel + 1];
            int toLayerIndex = layerIndex[currentLevel];
            int fromLayerSize = layerCounts[currentLevel + 1];
            int toLayerSize = layerFeedCounts[currentLevel];

            // clear the to-deltas
            for (int i = 0; i < fromLayerSize; i++)
            {
                layerDelta[fromLayerIndex + i] = 0;
            }

            int index = weightIndex[currentLevel];

            // handle weights
            for (int x = 0; x < toLayerSize; x++)
            {
                for (int y = 0; y < fromLayerSize; y++)
                {
                    float value = layerOutput[fromLayerIndex + y]
                            * layerDelta[toLayerIndex + x];
                    gradients[index+gradientOffset] += value;
                    layerDelta[fromLayerIndex + y] += weights[index]
                            * layerDelta[toLayerIndex + x];
                    index++;
                }
            }

            for (int i = 0; i < fromLayerSize; i++)
            {
                layerDelta[fromLayerIndex + i] *= derivative(
                  activationType[currentLevel+1],
                  layerOutput[fromLayerIndex + i], 1); 
            }
		}
	}	
}